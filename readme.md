# Decoding continuous goal-directed movement from human brain-wide intracranial recordings.
M.C Ottenhoff, M.V. Verwoert, S. Goulis, S. Tousseyn, J.P. van Dijk, M. Shanechi, O. Sani, P. Kubben and C. Herff

>**Abstract**
>
>Reaching out your hand is an effortless yet complex behavior that is indispensable in daily life. Restoring arm functionality is therefore rated a top priority by people with tetraplegia. Recently, neural correlates of movement have been observed and decoded beyond the motor cortex, but the degree and granularity of movement representation is not fully understood. Here, we explore the neural content of brain-wide movement-related neural activity by decoding neural correlates into 12 different kinematics of goal-directed reaching behavior. Eighteen participants implanted with stereotactic electroencephalography electrodes performed a gamified 3D goal-directed movement task. We demonstrate that continuous movement kinematics can be decoded from distributed recordings using low, mid and high frequency information in all participants using preferential subspace identification (PSID). The neural correlates of movement were distributed throughout the brain, including deeper structures such as the basal ganglia and insula. Moreover, we show that hand position could only be decoded using a goal-directed reference frame, indicating that widespread low-frequency activity is involved in higher-order processing of movements. Our results strengthen the evidence that widespread motor-related dynamics exist across numerous brain regions and can be used to continuously decode movement. The results may provide new opportunities for motor brain-computer interfaces for individuals with a compromised motor cortex, e.g. after stroke, or for control signals in adaptive closed-loop systems.

# Installation
Python version used: `3.11.9`

`git clone git@github.com:mottenhoff/decoding-continuous-goal-directed-movements.git`\
`cd decoding-continuous-goal-directed-movements`\
`pip install -r requirements.txt`

# Data availability
- Download the accompanying data here (released at publication)

# Running the analyis

**Decoder**:
1. Set path to the data in `main.py`\
2. The script can run with multiple options (e.g. different frequency bands, or including a target vector). These are set in the configs in  `configs/`. To run a config, for example delta activity with a common electrode reference: `python main.py ./configs/delta_cer.yml`. If no config is supplied, `main.py` will default to `./config.yml`, which the config file ran most recently. `main.py` will run each analysis of each participant in parallel in separate processes unless the `parallel` option is set to 0 in the config. The results for each process will be saved in a separate directory in `./results/<datetime>`. You may want to move and rename the result directories, for example: `./finished_runs/delta_cer`.
3. Or, to run everything: set path_data to you data directory in `run_all.sh` and then run `bash run_all.sh`

**Figures**:
To create the main figures in the paper:
1. Set the paths in `make_figures.py` to point to the data and to the results generated by the decoder.
2. Calculate the chance levels in `calculate_chance_levels.py`. Make sure to set the correct path in this file as well.
3. `python make_figures.py`


---
Plots to make the 3d brains used in the paper are available on request.

